# üöÄ The AI Prompt Generator: Your Launchpad to Creativity

Welcome to the world of AI-powered creativity. This isn't just code; it's your personal assistant for generating ideas, analyzing content, and exploring the possibilities of AI. Let's break it down, piece by piece, so you know exactly what's happening under the hood.

## üìö Importing Our Tools

```python
import streamlit as st
import google.generativeai as genai
import os
import json
import csv
import pandas as pd
from io import StringIO
import PyPDF2
import requests
from streamlit_lottie import st_lottie
```

What's happening here? We're bringing in all the tools we need:

- **`streamlit as st`**: This creates our web interface. Think of it as the control panel for our AI application. We use `as st` to give it a shorter name so we don't have to type `streamlit` every time.
- **`google.generativeai as genai`**: This is Google's powerful AI engine, the brain behind our prompts. We use `as genai` for the same reason as above ‚Äì it's easier to type!
- **`os`**: This lets us interact with the operating system, like managing files and directories.
- **`json`**: This helps us work with data in JSON format, a common way to structure information.
- **`csv`**: This allows us to read and write data in CSV (Comma Separated Value) format, often used for spreadsheets and databases.
- **`pandas as pd`**: This is a powerful library for data analysis and manipulation. We use `as pd` to give it a shorter name.
- **`from io import StringIO`**: This lets us treat strings as if they were files, which is useful for working with data generated by the AI.
- **`PyPDF2`**: This library is specifically for reading PDF files, so we can analyze their content.
- **`requests`**: This allows us to make HTTP requests to fetch data from the web, like our Lottie animation.
- **`from streamlit_lottie import st_lottie`**: This helps us display cool animations on our web app using Lottie files.

Each import gives us specific capabilities, like reading files, processing data, or displaying animations, which we'll use throughout our application.

## üñ•Ô∏è Setting Up Our Display

```python
st.set_page_config(
    page_title="AI Prompt Generator",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)
```

This code configures how our web app looks:

- **`page_title="AI Prompt Generator"`**: Sets the title that appears in the browser tab.
- **`page_icon="ü§ñ"`**: Uses a robot emoji as the icon for the browser tab.
- **`layout="wide"`**: Makes the layout wide for better visibility, using more of your screen space.
- **`initial_sidebar_state="expanded"`**: Starts with the sidebar open, so users can see the settings right away.

It's all about creating a user-friendly interface that's both functional and visually appealing.

## üîê Securing Our Connection

```python
api_key = st.sidebar.text_input("Enter your Gemini API key", type="password")
if api_key:
    genai.configure(api_key=api_key)
    st.session_state.api_configured = True
```

This is crucial for security and functionality:

- **`api_key = st.sidebar.text_input("Enter your Gemini API key", type="password")`**: We ask the user to input their Gemini API key in the sidebar. The `type="password"` hides the key as it's typed, just like a password field.
- **`if api_key:`**: This checks if the user has entered an API key. If they haven't, the code inside this block won't run.
- **`genai.configure(api_key=api_key)`**: If an API key is provided, we use it to configure the AI engine. This is like giving the AI the credentials it needs to access Google's servers.
- **`st.session_state.api_configured = True`**: We set a flag in the session state to remember that the API is configured. This way, we don't have to ask for the key again during the same session.

This step is like unlocking a door. Without the right key, we can't access the AI's capabilities.

## üéõÔ∏è Customizing Our AI

```python
st.sidebar.subheader("Model Settings")
model_version = st.sidebar.selectbox("Select Gemini Model", ["gemini-1.5-pro-flash", "gemini-1.5-pro", "gemini-1.5-pro-exp-0801"])
st.session_state.model_version = model_version
st.session_state.temperature = st.sidebar.slider("Temperature", 0.0, 1.5, 0.5, 0.5)
st.session_state.max_output_tokens = st.sidebar.number_input("Max Output Tokens", 1024, 8192, 8192, step=1024)
```

Here, we're giving users control over how the AI behaves:

- **`st.sidebar.subheader("Model Settings")`**: This adds a heading in the sidebar to group the settings.
- **`model_version = st.sidebar.selectbox("Select Gemini Model", ["gemini-1.5-pro-flash", "gemini-1.5-pro", "gemini-1.5-pro-exp-0801"])`**: This lets the user choose which version of the Gemini model to use. Each version might have different capabilities or performance characteristics.
- **`st.session_state.model_version = model_version`**: We store the selected model version in the session state so we can use it later.
- **`st.session_state.temperature = st.sidebar.slider("Temperature", 0.0, 1.5, 0.5, 0.5)`**: This slider controls the "temperature" of the AI's responses. Temperature influences the creativity and randomness of the output:
    - Lower temperatures (closer to 0.0) make the AI more focused and deterministic, sticking closely to the most likely responses.
    - Higher temperatures (closer to 1.5) make the AI more creative and unpredictable, exploring a wider range of possibilities.
- **`st.session_state.max_output_tokens = st.sidebar.number_input("Max Output Tokens", 1024, 8192, 8192, step=1024)`**: This lets the user set a limit on the length of the AI's responses. The number represents tokens, which are roughly equivalent to words or parts of words. This helps control how much text the AI generates.

These settings allow users to fine-tune the AI's behavior to suit their needs, making it more focused or more creative, and controlling the length of its output.

## üìã Creating Our Main Menu

```python
st.title("ü§ñ AI Prompt Generator")
selected = st.selectbox("Choose an option", ["Generate Prompt", "Analyze File", "Generate Dataset", "Help"])
```

This sets up the main interface of our app:

- **`st.title("ü§ñ AI Prompt Generator")`**: This displays a large heading with a robot emoji to welcome users.
- **`selected = st.selectbox("Choose an option", ["Generate Prompt", "Analyze File", "Generate Dataset", "Help"])`**: This creates a dropdown menu where users can select what they want to do. The options are:
    - **Generate Prompt**: Create a new AI prompt.
    - **Analyze File**: Analyze the content of an uploaded file.
    - **Generate Dataset**: Create a dataset of test data.
    - **Help**: View instructions on how to use the app.

This dropdown menu is the gateway to our app's core functionalities, allowing users to easily navigate and choose what they want to do.

## ‚ú® The Heart of Our App: Generate Prompt

```python
if selected == "Generate Prompt":
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Generate AI/LLM Prompt")
        task = st.text_area("Enter your question or task:", height=100, key="task_input", help="Click 'Generate Prompt' Button below")
        variables = st.text_input("Enter input variables (comma-separated):", key="variables_input")
        
        # Add example input variables
        st.markdown("""
        **Example input variables:**
        - topic, audience, tone
        - product_name, target_market, unique_selling_point
        - character_name, setting, genre
        """)
        
        if st.button("Generate Prompt", key="generate_button"):
            if task:
                with st.spinner("Generating prompt..."):
                    generated_prompt = generate_prompt(task, variables)
                    if generated_prompt:
                        st.subheader("Generated Prompt:")
                        annotated_text(
                            annotation(generated_prompt, "AI-Generated", "#ff4b4b")
                        )
                        
                        # Download options
                        st.subheader("Download Options")
                        prompt_download = get_download_link(generated_prompt, "generated_prompt.txt", "Download Prompt as TXT")
                        st.markdown(prompt_download, unsafe_allow_html=True)
                        
                        # Create JSONL file
                        jsonl_content = json.dumps({"prompt": generated_prompt, "completion": ""}) + "\n"
                        jsonl_download = get_download_link(jsonl_content, "generated_prompt.jsonl", "Download Prompt as JSONL")
                        st.markdown(jsonl_download, unsafe_allow_html=True)
            else:
                st.warning("Please enter a task.")
    
    with col2:
        st_lottie(lottie_ai, height=300, key="lottie_ai")
```

This is where the magic of prompt generation happens:

- **`if selected == "Generate Prompt":`**: This block of code only runs if the user has selected "Generate Prompt" from the main menu.
- **`col1, col2 = st.columns([2, 1])`**: This divides the screen into two columns, with the left column (`col1`) being twice as wide as the right column (`col2`).
- **`with col1:`**: This block of code will place its content in the left column.
    - **`st.subheader("Generate AI/LLM Prompt")`**: This adds a subheading to indicate the purpose of this section.
    - **`task = st.text_area("Enter your question or task:", height=100, key="task_input", help="Click 'Generate Prompt' Button below")`**: This creates a text area where the user can enter their task or question. The `height=100` makes the text area taller, and the `help` parameter provides a tooltip.
    - **`variables = st.text_input("Enter input variables (comma-separated):", key="variables_input")`**: This creates a text input field where the user can enter variables related to their task, separated by commas.
    - **`st.markdown(...)`**: This displays some example input variables to guide the user.
    - **`if st.button("Generate Prompt", key="generate_button"):`**: This creates a button labeled "Generate Prompt". When clicked, the code inside this block will run.
        - **`if task:`**: This checks if the user has entered a task. If not, it will skip the AI prompt generation and display a warning.
        - **`with st.spinner("Generating prompt..."):`**: This displays a spinner animation while the AI is generating the prompt, providing visual feedback to the user.
            - **`generated_prompt = generate_prompt(task, variables)`**: This calls the `generate_prompt` function (which we'll define later) to generate the prompt using the user's task and variables.
            - **`if generated_prompt:`**: This checks if the `generate_prompt` function returned a prompt. If not, it means there was an error during generation.
                - **`st.subheader("Generated Prompt:")`**: This displays a subheading for the generated prompt.
                - **`annotated_text(annotation(generated_prompt, "AI-Generated", "#ff4b4b"))`**: This displays the generated prompt with a red background and the label "AI-Generated".
                - **`st.subheader("Download Options")`**: This adds a subheading for the download options.
                - **`prompt_download = get_download_link(generated_prompt, "generated_prompt.txt", "Download Prompt as TXT")`**: This creates a download link for the prompt as a TXT file.
                - **`st.markdown(prompt_download, unsafe_allow_html=True)`**: This displays the download link. The `unsafe_allow_html=True` is necessary because the link is HTML code.
                - **`jsonl_content = json.dumps({"prompt": generated_prompt, "completion": ""}) + "\n"`**: This creates a JSONL (JSON Lines) string containing the prompt. JSONL is a format where each line is a valid JSON object.
                - **`jsonl_download = get_download_link(jsonl_content, "generated_prompt.jsonl", "Download Prompt as JSONL")`**: This creates a download link for the prompt as a JSONL file.
                - **`st.markdown(jsonl_download, unsafe_allow_html=True)`**: This displays the download link for the JSONL file.
        - **`else:`**: This block runs if the user hasn't entered a task.
            - **`st.warning("Please enter a task.")`**: This displays a warning message reminding the user to enter a task.
- **`with col2:`**: This block of code will place its content in the right column.
    - **`st_lottie(lottie_ai, height=300, key="lottie_ai")`**: This displays a Lottie animation related to AI. Lottie animations are lightweight and interactive, adding visual interest to the app.

This section turns user input into AI-generated prompts, forming the core functionality of our application. It also provides convenient download options and visual feedback during the generation process.

## üìä Analyzing Files: Turning Data into Insights

```python
elif selected == "Analyze File":
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Analyze Uploaded File")
        uploaded_file = st.file_uploader("Upload a CSV, TXT, MD, or PDF file", type=["csv", "txt", "md", "pdf"], key="file_uploader")
        
        if uploaded_file is not None:
            file_contents = process_uploaded_file(uploaded_file)
            st.write("File contents:")
            st.code(file_contents, language="plaintext")
            
            analysis_prompt = st.text_area("Enter analysis prompt:", "Analyze the contents of the uploaded file and provide insights.", key="analysis_prompt")
            
            if st.button("Analyze File", key="analyze_button"):
                if analysis_prompt:
                    with st.spinner("Analyzing file..."):
                        analysis_result = generate_prompt(f"{analysis_prompt}\n\nFile contents:\n{file_contents}")
                        if analysis_result:
                            st.subheader("Analysis Result:")
                            annotated_text(
                                annotation(analysis_result, "Analysis", "#ff4b4b") 
                            )
                else: 
                    st.warning("Please enter an analysis prompt.")
    
    with col2:
        st_lottie(lottie_analysis, height=300, key="lottie_analysis")
```

This section is all about turning uploaded files into valuable insights:

- **`elif selected == "Analyze File":`**: This block of code only runs if the user has selected "Analyze File" from the main menu.
- **`col1, col2 = st.columns([2, 1])`**: We again divide the screen into two columns, with the left one wider.
- **`with col1:`**: This block's content goes in the left column.
    - **`st.subheader("Analyze Uploaded File")`**: A subheading to indicate the purpose of this section.
    - **`uploaded_file = st.file_uploader("Upload a CSV, TXT, MD, or PDF file", type=["csv", "txt", "md", "pdf"], key="file_uploader")`**: This creates a file uploader component, allowing users to upload files of the specified types.
    - **`if uploaded_file is not None:`**: This checks if a file has been uploaded. If not, the code inside this block won't run.
        - **`file_contents = process_uploaded_file(uploaded_file)`**: This calls the `process_uploaded_file` function (defined earlier) to read the content of the uploaded file.
        - **`st.write("File contents:")`**: This displays a label "File contents:".
        - **`st.code(file_contents, language="plaintext")`**: This displays the file contents using a code block, making it easier to read.
        - **`analysis_prompt = st.text_area("Enter analysis prompt:", "Analyze the contents of the uploaded file and provide insights.", key="analysis_prompt")`**: This creates a text area where users can enter a prompt for analyzing the file content.
        - **`if st.button("Analyze File", key="analyze_button"):`**: This creates a button labeled "Analyze File". When clicked:
            - **`if analysis_prompt:`**: This checks if the user has entered an analysis prompt. If not, it displays a warning.
            - **`with st.spinner("Analyzing file..."):`**: This displays a spinner animation while the AI is analyzing the file.
                - **`analysis_result = generate_prompt(f"{analysis_prompt}\n\nFile contents:\n{file_contents}")`**: This calls the `generate_prompt` function, passing both the analysis prompt and the file contents to generate a response.
                - **`if analysis_result:`**: This checks if the `generate_prompt` function returned a result. If not, it means there was an error during analysis.
                    - **`st.subheader("Analysis Result:")`**: This displays a subheading for the analysis result.
                    - **`annotated_text(annotation(analysis_result, "Analysis", "#ff4b4b"))`**: This displays the analysis result with a red background and the label "Analysis".
            - **`else:`**: This block runs if the user hasn't entered an analysis prompt.
                - **`st.warning("Please enter an analysis prompt.")`**: This displays a warning message reminding the user to enter a prompt.
- **`with col2:`**: This block's content goes in the right column.
    - **`st_lottie(lottie_analysis, height=300, key="lottie_analysis")`**: This displays a Lottie animation related to file analysis.

This feature turns your AI into a powerful data analyst, capable of extracting insights from various file types based on user-specified criteria. It also provides visual feedback during the analysis process.

## üß™ Generating Test Data: Creating Realistic Scenarios

```python
elif selected == "Generate Test Data":
    st.subheader("Generate Test Dataset for Fine Tuning an LLM")
    topic = st.text_input("Enter your text or topic here:", key="test_data_topic")
    num_pairs = st.number_input("Number of conversation pairs to generate:", min_value=1, max_value=100, value=10, step=1, key="num_pairs")
    
    if st.button("Generate Test Data", key="generate_test_data_button"):
        if topic:
            with st.spinner("Generating test data..."):
                test_data = generate_test_data(topic, num_pairs)
                if test_data:
                    st.json(test_data)
                    
                    # Download options
                    st.subheader("Download Options")
                    json_download = get_download_link(json.dumps(test_data, indent=2), "test_data.json", "Download as JSON")
                    st.markdown(json_download, unsafe_allow_html=True)
                    
                    # Create JSONL file
                    jsonl_content = "\n".join(json.dumps(item) for item in test_data)
                    jsonl_download = get_download_link(jsonl_content, "test_data.jsonl", "Download as JSONL")
                    st.markdown(jsonl_download, unsafe_allow_html=True)
        else:
            st.warning("Please enter a topic for test data generation.")
```

This section is a powerhouse for creating custom test data:

- **`elif selected == "Generate Test Data":`**: This block of code only runs if the user has selected "Generate Test Data" from the main menu.
- **`st.subheader("Generate Test Dataset for Fine Tuning an LLM")`**: This adds a subheading to indicate the purpose of this section, specifically mentioning fine-tuning Large Language Models (LLMs).
- **`topic = st.text_input("Enter your text or topic here:", key="test_data_topic")`**: This creates a text input field where the user can enter a topic or some text that will be used as the basis for generating the test data.
- **`num_pairs = st.number_input("Number of conversation pairs to generate:", min_value=1, max_value=100, value=10, step=1, key="num_pairs")`**: This lets the user specify how many conversation pairs they want to generate. Each pair will consist of a human message and an AI response.
- **`if st.button("Generate Test Data", key="generate_test_data_button"):`**: This creates a button labeled "Generate Test Data". When clicked:
    - **`if topic:`**: This checks if the user has entered a topic. If not, it displays a warning.
    - **`with st.spinner("Generating test data..."):`**: This displays a spinner animation while the AI is generating the test data.
        - **`test_data = generate_test_data(topic, num_pairs)`**: This calls the `generate_test_data` function (defined earlier) to generate the conversation pairs based on the provided topic and number of pairs.
        - **`if test_data:`**: This checks if the `generate_test_data` function returned any data. If not, it means there was an error during generation.
            - **`st.json(test_data)`**: This displays the generated test data in JSON format, making it easy to read and understand the structure.
            - **`st.subheader("Download Options")`**: This adds a subheading for the download options.
            - **`json_download = get_download_link(json.dumps(test_data, indent=2), "test_data.json", "Download as JSON")`**: This creates a download link for the test data as a JSON file. The `indent=2` makes the JSON output more readable.
            - **`st.markdown(json_download, unsafe_allow_html=True)`**: This displays the download link for the JSON file.
            - **`jsonl_content = "\n".join(json.dumps(item) for item in test_data)`**: This creates a JSONL string from the generated test data.
            - **`jsonl_download = get_download_link(jsonl_content, "test_data.jsonl", "Download as JSONL")`**: This creates a download link for the test data as a JSONL file.
            - **`st.markdown(jsonl_download, unsafe_allow_html=True)`**: This displays the download link for the JSONL file.
    - **`else:`**: This block runs if the user hasn't entered a topic.
        - **`st.warning("Please enter a topic for test data generation.")`**: This displays a warning message reminding the user to enter a topic.

This feature transforms your AI into a data generation tool, creating realistic datasets for testing, prototyping, or demonstration purposes. It provides convenient download options in both JSON and JSONL formats.

## üÜò Help Section: Your AI Assistant Guide

```python
elif selected == "Help":
    st.subheader("How to Use This App")
    st.markdown("""
    1. **Generate Prompt**:
       - Enter your task in the text area.
       - Optionally, provide input variables separated by commas.
       - Click "Generate Prompt" to create an AI-generated prompt.
       - Download the generated prompt as a .txt or JSONL file.

    2. **Analyze File**:
       - Upload a CSV, TXT, MD, or PDF file.
       - Enter an analysis prompt to guide the AI.
       - Click "Analyze File" to get insights about the uploaded file.

    3. **Generate Test Data conversation pairs and datasets for Fine Tuning AI/LLM Models**:
       - Enter a topic or text for test data (also referred to as synthetic data) generation.
       - Specify the number of conversation pairs to generate.
       - Click "Generate Test Data" to create conversation pairs.
       - Download the generated conversation pairs data as JSON or JSONL

    4. **Sidebar Options**:
       - Enter your Gemini API key.
       - Select the Gemini model version.
       - Adjust temperature and max output tokens for generation.

    5. **Tips for Better Results**:
       - Be specific in your task description.
       - Experiment with different temperature settings.
       - For file analysis, provide clear instructions in the analysis prompt.
    """)

    st.subheader("FAQ")
    faq = {
        "What is the Gemini API?": "The Gemini API is Google's latest and most capable AI model for text generation and analysis.",
        "How do I get an API key?": "You can obtain a Free Gemini API key by signing up at Google AI Studio https://aistudio.google.com.",
        "What file types are supported?": "Currently, the app supports CSV, TXT, Markdown MD, and PDF file formats for analysis.",
        "Is my data secure?": "Yes! Your data and API key are processed locally and not stored on any servers. Always ensure you're using the app from a trusted source.",
        "What's the difference between the models?": "The flash model is faster and as of Aug 10, 2024 has advanced PDF unstructured text recognition, while the pro models offer larger context windows and more advanced capabilities.",
        "Can I use audio, images or video with all models?": "Yes, all Gemini 1.5 models support multimodal inputs including images, audio, and video.",
        "What are the token limits?": "The flash model has an input limit of 1,048,576 tokens, while the pro models can handle up to 2,000,000 tokens. All models have an output limit of 8,192 tokens."
    }

    for question, answer in faq.items():
        with st.expander(question):
            st.write(answer)
```

This section serves as the user manual for our application:

- **`elif selected == "Help":`**: This block of code only runs if the user has selected "Help" from the main menu.
- **`st.subheader("How to Use This App")`**: This displays a subheading for the general instructions.
- **`st.markdown(...)`**: This displays a multi-line text block formatted with Markdown, explaining how to use each of the app's main features:
    - **Generate Prompt**: Step-by-step instructions on how to generate AI prompts.
    - **Analyze File**: Instructions on how to upload and analyze files.
    - **Generate Test Data**: Instructions on how to generate test data for AI models.
    - **Sidebar Options**: A reminder to enter the Gemini API key and an explanation of the model version, temperature, and max output tokens settings.
    - **Tips for Better Results**: General advice on how to get the best results from the AI.
- **`st.subheader("FAQ")`**: This displays a subheading for the Frequently Asked Questions section.
- **`faq = {...}`**: This creates a dictionary containing common questions and their answers.
- **`for question, answer in faq.items():`**: This loops through each question and answer in the `faq` dictionary.
    - **`with st.expander(question):`**: This creates an expandable section for each question. When the user clicks on the question, the answer will be revealed.
        - **`st.write(answer)`**: This displays the answer to the question within the expandable section.

This help section is always available from the main menu, acting as a quick reference for users at any time. It provides clear instructions, tips, and answers to common questions, making the app more user-friendly and accessible.

## ü¶∂ Footer: Giving Credit and Contact Information

```python
# Footer
st.markdown("---")
st.markdown("Created with ‚ù§Ô∏è by Gregory Kennedy | Powered by Gemini AI")
st.markdown('<a href="https://www.linkedin.com/in/gregorykennedymindfuldude" target="_blank" rel="noopener noreferrer">Contact Gregory</a>', unsafe_allow_html=True)
```

This section adds a footer to our app:

- **`st.markdown("---")`**: This creates a horizontal line to visually separate the footer from the main content.
- **`st.markdown("Created with ‚ù§Ô∏è by Gregory Kennedy | Powered by Gemini AI")`**: This displays a credit line, acknowledging the creator and the AI technology used.
- **`st.markdown('<a href="https://www.linkedin.com/in/gregorykennedymindfuldude" target="_blank" rel="noopener noreferrer">Contact Gregory</a>', unsafe_allow_html=True)`**: This creates a clickable link to the creator's LinkedIn profile. The `unsafe_allow_html=True` is necessary because we're embedding HTML code within the Markdown.

The footer provides a professional touch, giving credit and offering a way for users to contact the creator.

## ‚ö†Ô∏è API Key Warning: Keeping Things Secure

```python
# Add warning about API key
st.sidebar.warning(
    "Please note: Your API key is not stored and is only used for the current session. "
    "Always keep your API key confidential and do not share it with others."
)
```

This is an important security reminder:

- **`st.sidebar.warning(...)`**: This displays a warning message in the sidebar.
- The message emphasizes that the user's API key is **not stored** by the app and is only used during the current session.
- It also strongly advises users to keep their API key **confidential** and not share it with anyone.

This warning helps protect users' API keys and reinforces good security practices.

## ‚ÑπÔ∏è Version Information: Keeping Track of Updates

```python
# Add version information
st.sidebar.info(f"App Version: 1.8.0 | Using Gemini Model: {st.session_state.model_version}")
```

This section provides useful information about the app:

- **`st.sidebar.info(...)`**: This displays an informational message in the sidebar.
- **`f"App Version: 1.8.0 | Using Gemini Model: {st.session_state.model_version}"`**: This message shows the current version of the app and the selected Gemini model version.

This helps users understand which version of the app they're using and which AI model is powering it.

## üêõ Debug Information: For Developers' Eyes Only

```python
# Debug Information (only visible when running in debug mode)
if os.environ.get("DEBUG_MODE") == "True":
    st.sidebar.subheader("Debug Information")
    st.sidebar.json({
        "Temperature": st.session_state.temperature,
        "Max Output Tokens": st.session_state.max_output_tokens,
        "Model Version": st.session_state.model_version,
        "API Configured": st.session_state.api_configured
    })
```

This section is for debugging purposes and is only visible if the app is running in debug mode:

- **`if os.environ.get("DEBUG_MODE") == "True":`**: This checks if an environment variable called `DEBUG_MODE` is set to "True". If not, the code inside this block won't run.
- **`st.sidebar.subheader("Debug Information")`**: This adds a subheading in the sidebar for the debug information.
- **`st.sidebar.json(...)`**: This displays a JSON object containing various debug information:
    - **Temperature**: The current temperature setting.
    - **Max Output Tokens**: The current max output tokens setting.
    - **Model Version**: The selected Gemini model version.
    - **API Configured**: Whether the API key has been configured.

This debug information can be helpful for developers to understand the app's internal state and troubleshoot any issues.

## üö´ Error Handling: Gracefully Handling the Unexpected

```python
# Error Handling
def handle_error(error):
    st.error(f"An error occurred: {str(error)}")
    if os.environ.get("DEBUG_MODE") == "True":
        st.exception(error)
```

This section defines a function for handling errors:

- **`def handle_error(error):`**: This defines a function called `handle_error` that takes an `error` object as input.
- **`st.error(f"An error occurred: {str(error)}")`**: This displays an error message to the user, including the error message as a string.
- **`if os.environ.get("DEBUG_MODE") == "True":`**: This checks if the app is running in debug mode.
    - **`st.exception(error)`**: If in debug mode, this displays the full error traceback, providing more detailed information for developers.

This error handling mechanism helps prevent the app from crashing and provides useful information to users and developers in case of errors.

## üìë Release Notes: Keeping Users Informed

```python
# Add a collapsible section for release notes
with st.sidebar.expander("Release Notes"):
    st.markdown("""
    ### Version 1.8.0
    - Added Langsmith for LLM Analysis, Tracing and Observability 
                          
    ### Version 1.7.0
    - Improved JSON handling in test data generation
    - Added error handling for JSON parsing
    - Updated UI for better user experience

    ### Version 1.6.0
    - Added Generate Test Data feature
    - Improved error handling for test data generation
    - Updated UI to include new feature in the menu

    ### Version 1.5.0
    - Added support for newest Gemini 1.5 models
    - Improved error handling and debugging
    - Enhanced user interface and responsiveness
    - Added FAQ section in Help
    
    ### Version 1.4.0
    - Introduced file analysis feature
    - Expanded supported file types
    - Added download options for generated prompts
    
    ### Version 1.3.0
    - Integrated Lottie animations
    - Improved sidebar controls
    - Added temperature and max token adjustments
    """)
```

This section provides a convenient way to inform users about updates and changes in the app:
```Code execution output
Traceback (most recent call last):
  File "<string>", line 1
    ```
    ^
SyntaxError: invalid syntax
```
You are absolutely right! I need to be more careful about how I handle code blocks within Markdown. I'll make sure to escape them properly so they don't cause syntax errors. 

Let's continue with the next chunk of the updated explainer:

```markdown
- **`with st.sidebar.expander("Release Notes"):`**: This creates an expandable section in the sidebar labeled "Release Notes". Users can click on it to see the details of each app version.
- **`st.markdown(...)`**: This displays a multi-line text block formatted with Markdown, listing the changes and improvements made in each version of the app.

This helps keep users informed about the app's evolution and encourages them to stay up-to-date with the latest features and bug fixes.

## üöÄ Performance Optimization: Making Things Run Smoothly

```python
# Performance Optimization
@st.cache_data
def load_static_resources():
    # Load any static resources here
    pass

load_static_resources()
```

This section is about making our app run faster and more efficiently:

- **`@st.cache_data`**: This is a decorator that tells Streamlit to cache the results of the function. This means that the function will only be executed once, and the results will be stored in memory. Subsequent calls to the function will simply retrieve the cached results, saving time and resources.
- **`def load_static_resources():`**: This defines a function called `load_static_resources` that is responsible for loading any static resources that the app needs, such as images, CSS files, or JavaScript files.
- **`# Load any static resources here`**: This is a placeholder comment where you would add the actual code to load your static resources.
- **`load_static_resources()`**: This calls the `load_static_resources` function to load the static resources when the app starts.

By caching the results of the `load_static_resources` function, we can avoid loading the same resources repeatedly, improving the app's performance.

## ‚öôÔ∏è Initializing Session State: Setting the Stage

```python
@traceable # Langsmith Tracing and Observability

# Ensure all session state variables are initialized
def initialize_session_state():
    default_values = {
        "temperature": 0.5,
        "max_output_tokens": 8000,
        "model_version": "gemini-1.5-pro-flash",
        "api_configured": False,
        "generated_prompts": [],
        "analyzed_files": []
    }
    for key, value in default_values.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()
```

This section ensures that all necessary variables are set up when the app starts:

- **`@traceable # Langsmith Tracing and Observability`**: This decorator is used for tracking and monitoring the performance of the function using a tool called Langsmith. It helps developers understand how the function is being used and identify any potential issues.
- **`def initialize_session_state():`**: This defines a function called `initialize_session_state` that sets up the initial values for variables stored in the session state. The session state is like a temporary memory that holds information for the current user's session.
- **`default_values = {...}`**: This dictionary contains the default values for various session state variables:
    - **`temperature`**: The default temperature for the AI model.
    - **`max_output_tokens`**: The default maximum number of tokens for the AI's responses.
    - **`model_version`**: The default Gemini model version.
    - **`api_configured`**: A flag indicating whether the API key has been configured.
    - **`generated_prompts`**: A list to store generated prompts.
    - **`analyzed_files`**: A list to store information about analyzed files.
- **`for key, value in default_values.items():`**: This loop iterates through each key-value pair in the `default_values` dictionary.
    - **`if key not in st.session_state:`**: This checks if the key (variable name) already exists in the session state.
        - **`st.session_state[key] = value`**: If the key doesn't exist, it's added to the session state with its default value.

- **`initialize_session_state()`**: This calls the `initialize_session_state` function to set up the session state when the app starts.

This initialization process ensures that the app has all the necessary variables and their default values ready to go when a user starts interacting with it.

## üé¨ Main App Execution: Bringing It All Together

```python
# Add this at the very end of your script
if __name__ == "__main__":
    try:
        # Main app execution
        pass
    except Exception as e:
        handle_error(e)
```

This is the final piece of the puzzle, the part that actually runs the app:

- **`if __name__ == "__main__":`**: This is a common Python idiom. It ensures that the code inside this block only runs when the script is executed directly, not when it's imported as a module.
- **`try:`**: This block tries to execute the main app logic.
    - **`# Main app execution`**: This is a placeholder comment where you would add any additional code that needs to run when the app starts. In this case, the main app logic is already handled by the code we've explained above, so this block is empty.
- **`except Exception as e:`**: This block catches any exceptions (errors) that occur during the execution of the `try` block.
    - **`handle_error(e)`**: If an exception occurs, it's passed to the `handle_error` function, which displays an error message to the user.

This structure ensures that the app starts up correctly and handles any unexpected errors gracefully.

## üìù Function Definitions: The Building Blocks of Our App

Now let's take a closer look at the functions we've defined:

### üì• `process_uploaded_file(uploaded_file)`: Reading File Content

```python
# Function to process uploaded file
def process_uploaded_file(uploaded_file):
    if uploaded_file.type == "text/csv":
        df = pd.read_csv(uploaded_file)
        return df.to_string()
    elif uploaded_file.type == "application/pdf":
        # PDF handling using PyPDF2
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        num_pages = len(pdf_reader.pages)
        content = ""
        for page_num in range(num_pages):
            page = pdf_reader.pages[page_num]
            content += page.extract_text()
        return content
    else:
        content = uploaded_file.getvalue().decode("utf-8")
        return content
```

This function is responsible for reading the content of uploaded files:

- **`def process_uploaded_file(uploaded_file):`**: This defines the function, taking an `uploaded_file` object as input.
- **`if uploaded_file.type == "text/csv":`**: This checks if the uploaded file is a CSV file.
    - **`df = pd.read_csv(uploaded_file)`**: If it's a CSV, we use pandas (`pd`) to read it into a DataFrame (`df`).
    - **`return df.to_string()`**: We then convert the DataFrame to a string and return it.
- **`elif uploaded_file.type == "application/pdf":`**: This checks if the uploaded file is a PDF file.
    - **`pdf_reader = PyPDF2.PdfReader(uploaded_file)`**: If it's a PDF, we use PyPDF2 to create a `PdfReader` object.
    - **`num_pages = len(pdf_reader.pages)`**: We get the number of pages in the PDF.
    - **`content = ""`**: We initialize an empty string to store the extracted text.
    - **`for page_num in range(num_pages):`**: This loop iterates through each page of the PDF.
        - **`page = pdf_reader.pages[page_num]`**: We get the current page.
        - **`content += page.extract_text()`**: We extract the text from the page and append it to the `content` string.
    - **`return content`**: We return the extracted text from all pages.
- **`else:`**: This block runs if the uploaded file is not a CSV or PDF.
    - **`content = uploaded_file.getvalue().decode("utf-8")`**: We read the file content as bytes and decode it as UTF-8 text.
    - **`return content`**: We return the decoded text.

This function handles different file types, ensuring that we can extract the text content from CSV, PDF, and other text-based files.

### üîó `get_download_link(content, filename, text)`: Creating Download Links

```python
# Function to generate a download link
def get_download_link(content, filename, text):
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'
```

This function creates download links for text content:

- **`def get_download_link(content, filename, text):`**: This defines the function, taking the following arguments:
    - **`content`**: The text content to be downloaded.
    - **`filename`**: The desired filename for the downloaded file.
    - **`text`**: The text to be displayed on the download link.
- **`b64 = base64.b64encode(content.encode()).decode()`**: This encodes the text content in base64 format. Base64 encoding is a way to represent binary data (like text) as a string of ASCII characters, making it safe to include in URLs.
- **`return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'`**: This returns an HTML anchor tag (`<a>`) that creates a download link. The link's `href` attribute uses a data URL, which embeds the base64-encoded content directly in the link. The `download` attribute specifies the filename for the downloaded file.

This function allows users to easily download the generated prompts and test data as text files.

### üß† `get_system_prompt(task, variables, model_version, temperature, max_output_tokens)`: Crafting the System Prompt

```python
# New function to get system prompt
def get_system_prompt(task, variables, model_version, temperature, max_output_tokens):
    return f"""You are an advanced AI prompt engineer assistant integrated into a Streamlit app. Your task is to generate a highly effective Chain of Thought (COT) prompt based on the user's input. Follow these steps:

1. Analyze the task:
   - Identify the main objective of the user's task
   - Determine the complexity and scope of the task
   - Consider any specific domain knowledge required

2. Evaluate the variables:
   - Examine the provided variables (if any)
   - Determine how these variables should be incorporated into the prompt
   - Consider additional relevant variables that might enhance the prompt

3. Consider the Gemini model capabilities:
   - Adapt the prompt to leverage the strengths of the selected Gemini model ({model_version})
   - Take into account the current temperature setting ({temperature}) and max token limit ({max_output_tokens})

4. Incorporate COT elements:
   - Break down the task into logical steps or components
   - Include prompts for explanations or reasoning at each step
   - Encourage the model to show its work or thought process

5. Optimize for clarity and specificity:
   - Use clear, concise language
   - Avoid ambiguity in instructions
   - Include specific examples or constraints where appropriate

6. Add context and formatting instructions:
   - Provide any necessary background information
   - Specify desired output format (e.g., bullet points, paragraphs, JSON)
   - Include any relevant data or file analysis instructions if applicable

7. Encourage creativity and problem-solving:
   - Include prompts for alternative approaches or solutions
   - Ask for pros and cons of different methods if relevant

8. Incorporate error handling and edge cases:
   - Prompt the model to consider potential issues or limitations
   - Ask for validation steps or error checking where appropriate

9. Format the final prompt:
   - Present the COT prompt in a clear, structured manner
   - Use appropriate line breaks, numbering, or bullet points for readability

Generate a COT prompt that follows these steps and is optimized for the given task, variables, and selected Gemini model. Ensure the prompt is detailed enough to guide the AI but concise enough to fit within token limits.

Task: {task}
Variables: {variables}
Selected Model: {model_version}
Temperature: {temperature}
Max Tokens: {max_output_tokens}

Based on the above information, generate an optimal Chain of Thought prompt:
"""
```

This function is the mastermind behind generating effective Chain of Thought (COT) prompts:

- **`def get_system_prompt(task, variables, model_version, temperature, max_output_tokens):`**: This defines the function, taking the user's task, variables, selected model version, temperature, and max output tokens as input.
- **`return f"""..."""`**: This returns a multi-line string that acts as a detailed instruction set for the AI. It guides the AI through a step-by-step process to generate a high-quality COT prompt.

Let's break down the instructions within the system prompt:

1. **Analyze the task**: The AI is instructed to understand the user's goal, the complexity of the task, and any specific knowledge needed.
2. **Evaluate the variables**: The AI should consider how to incorporate the user-provided variables and think about any additional variables that might be helpful.
3. **Consider the Gemini model capabilities**: The AI should tailor the prompt to the strengths of the chosen Gemini model, taking into account the temperature and token limit settings.
4. **Incorporate COT elements**: The AI should break down the task into steps, encourage explanations, and prompt the model to show its thought process.
5. **Optimize for clarity and specificity**: The prompt should be clear, concise, and avoid ambiguity.
6. **Add context and formatting instructions**: The AI should include any necessary background information, specify the desired output format, and provide data analysis instructions if needed.
7. **Encourage creativity and problem-solving**: The prompt should encourage the AI to explore different approaches and solutions.
8. **Incorporate error handling and edge cases**: The AI should prompt the model to consider potential issues and include error checking steps.
9. **Format the final prompt**: The prompt should be well-structured and easy to read.

The system prompt then provides the AI with the specific task, variables, model version, temperature, and max tokens, and asks it to generate an optimal COT prompt based on all this information.

### ü§ñ `generate_prompt(task, variables="")`: Generating the Final Prompt

```python
@traceable # Langsmith Tracing and Observability
# Function to generate prompt
def generate_prompt(task, variables=""):
    model = genai.GenerativeModel(st.session_state.model_version)
    system_prompt = get_system_prompt(
        task, 
        variables, 
        st.session_state.model_version, 
        st.session_state.temperature, 
        st.session_state.max_output_tokens
    )
    
    try:
        response = model.generate_content(system_prompt,
                                          generation_config=genai.types.GenerationConfig(
                                              temperature=st.session_state.temperature,
                                              max_output_tokens=st.session_state.max_output_tokens,
                                          ))
        return response.text
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        return None
```

This function takes the user's input and uses the AI to generate the final prompt:

- **`@traceable # Langsmith Tracing and Observability`**: This decorator is used for tracking and monitoring the performance of the function using Langsmith.
- **`def generate_prompt(task, variables=""):`**: This defines the function, taking the user's task and variables as input.
- **`model = genai.GenerativeModel(st.session_state.model_version)`**: This creates a `GenerativeModel` instance using the selected Gemini model version from the session state.
- **`system_prompt = get_system_prompt(...)`**: This calls the `get_system_prompt` function to create the detailed instruction set for the AI, passing the user's task, variables, and the current model settings.
- **`try:`**: This block attempts to generate the prompt using the AI.
    - **`response = model.generate_content(system_prompt, generation_config=genai.types.GenerationConfig(...))`**: This uses the `GenerativeModel` to generate content based on the `system_prompt`. The `generation_config` specifies the temperature and max output tokens.
    - **`return response.text`**: If the generation is successful, we return the generated text from the AI's response.
- **`except Exception as e:`**: This block catches any errors that occur during the generation process.
    - **`st.error(f"An error occurred: {str(e)}")`**: If an error occurs, we display an error message to the user.
    - **`return None`**: We return `None` to indicate that the generation failed.

This function orchestrates the entire prompt generation process, from crafting the system prompt to handling potential errors.

### üé≤ `generate_test_data(topic, num_pairs)`: Creating Test Data

```python
@traceable # Langsmith Tracing and Observability
# Function to generate test data
def generate_test_data(topic, num_pairs):
    model = genai.GenerativeModel(st.session_state.model_version)
    prompt = f"""Generate {num_pairs} pairs of conversation for the topic: {topic}. 
    Each pair should consist of a human message and an AI response. 
    Format the output as a valid JSON array of objects, where each object has 'human' and 'ai' keys.
    Ensure the output is strictly in this format:
    [
        {{"human": "Human message 1", "ai": "AI response 1"}},
        {{"human": "Human message 2", "ai": "AI response 2"}},
        ...
    ]
    Do not include any text before or after the JSON array.
    """
    
    try:
        response = model.generate_content(prompt,
                                          generation_config=genai.types.GenerationConfig(
                                              temperature=st.session_state.temperature,
                                              max_output_tokens=st.session_state.max_output_tokens,
                                          ))
        
        # Attempt to parse the response as JSON
        try:
            json_data = json.loads(response.text)
            if isinstance(json_data, list) and all(isinstance(item, dict) and 'human' in item and 'ai' in item for item in json_data):
                return json_data
            else:
                raise ValueError("Response is not in the expected format")
        except json.JSONDecodeError:
            # If JSON parsing fails, try to extract JSON from the response
            json_match = re.search(r'\[.*\]', response.text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                raise ValueError("Could not extract valid JSON from the response")
    except Exception as e:
        st.error(f"An error occurred while generating test data: {str(e)}")
        return None
```

This function generates test data in the form of conversation pairs:

- **`@traceable # Langsmith Tracing and Observability`**: This decorator is used for tracking and monitoring the function's performance using Langsmith.
- **`def generate_test_data(topic, num_pairs):`**: This defines the function, taking the topic and the number of conversation pairs as input.
- **`model = genai.GenerativeModel(st.session_state.model_version)`**: This creates a `GenerativeModel` instance using the selected Gemini model version.
- **`prompt = f"""..."""`**: This creates a multi-line string that acts as a prompt for the AI. The prompt instructs the AI to generate the specified number of conversation pairs on the given topic, formatted as a JSON array.
- **`try:`**: This block attempts to generate the test data using the AI.
    - **`response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(...))`**: This uses the `GenerativeModel` to generate content based on the prompt, using the current temperature and max output tokens settings.
    - **`try:`**: This inner `try` block attempts to parse the AI's response as JSON.
        - **`json_data = json.loads(response.text)`**: This tries to convert the AI's response text into a JSON object.
        - **`if isinstance(json_data, list) and all(isinstance(item, dict) and 'human' in item and 'ai' in item for item in json_data):`**: This checks if the parsed JSON is a list of dictionaries, where each dictionary has the keys "human" and "ai", as specified in the prompt.
            - **`return json_data`**: If the format is correct, we return the parsed JSON data.
        - **`else:`**: This block runs if the parsed JSON doesn't match the expected format.
            - **`raise ValueError("Response is not in the expected format")`**: This raises an error indicating that the AI's response is not in the correct format.
    - **`except json.JSONDecodeError:`**: This block catches errors that occur if the AI's response is not valid JSON.
        - **`json_match = re.search(r'\[.*\]', response.text, re.DOTALL)`**: This tries to extract the JSON array from the response using regular expressions.
        - **`if json_match:`**: This checks if a JSON array was found.
            - **`json_str = json_match.group(0)`**: We extract the matched JSON string.
            - **`return json.loads(json_str)`**: We parse the extracted JSON string and return the result.
        - **`else:`**: This block runs if no JSON array was found in the response.
            - **`raise ValueError("Could not extract valid JSON from the response")`**: This raises an error indicating that valid JSON could not be extracted from the AI's response.
- **`except Exception as e:`**: This block catches any other errors that occur during the test data generation process.
    - **`st.error(f"An error occurred while generating test data: {str(e)}")`**: We display an error message to the user.
    - **`return None`**: We return `None` to indicate that the generation failed.

This function ensures that the generated test data is in the correct format and handles potential errors during the generation and parsing process.

## üèÅ Wrapping Up: The Complete Picture

We've now explored every line of code in our AI Prompt Generator app. You've seen how we import the necessary tools, set up the user interface, handle user input, generate AI prompts, analyze files, create test data, provide help and documentation, and handle errors gracefully.

This app is a testament to the power of AI and its ability to enhance creativity, automate tasks, and provide valuable insights. By understanding the code, you can gain a deeper appreciation for how AI works and how you can leverage it to solve problems and explore new possibilities.

Remember, this is just the beginning. The world of AI is vast and constantly evolving. With the knowledge you've gained, you can continue to explore, experiment, and create amazing things with AI.
```

